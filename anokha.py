# -*- coding: utf-8 -*-
"""Anokha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10r4_sJFEP_-y2zMJ2YvFaJdyZOtQwZtd
"""

import tensorflow as tf
tf.config.list_physical_devices('GPU')

import tensorflow as tf
print(tf.test.is_gpu_available())

!unzip AIoT26NDZ.zip

!ls /content

!pip install librosa soundfile tensorflow

import librosa
import numpy as np
import os

DATASET_DIR = "/content/AIoT26ND"
SAMPLE_RATE = 16000
N_MFCC = 13

X = []
y = []

label_map = {
    "dataset_hunger": 0,
    "dataset_pdiscomfort": 1,
    "dataset_edistress": 2
}

for label, idx in label_map.items():
    folder = os.path.join(DATASET_DIR, label)

    for file in os.listdir(folder):
        if not file.endswith(".wav"):
            continue

        file_path = os.path.join(folder, file)
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)

        mfcc = librosa.feature.mfcc(
            y=audio,
            sr=sr,
            n_mfcc=N_MFCC
        )

        mfcc = mfcc.T  # (time, features)
        X.append(mfcc)
        y.append(idx)
MAX_LEN = 100  # fixed number of time steps

def pad_mfcc(mfcc, max_len):
    if mfcc.shape[0] < max_len:
        return np.pad(mfcc, ((0, max_len - mfcc.shape[0]), (0, 0)))
    else:
        return mfcc[:max_len, :]

X = np.array([pad_mfcc(m, MAX_LEN) for m in X])
y = np.array(y)

print(X.shape, y.shape)

!ls /content/AIoT26NDZ

!ls /content

!ls /content/AIoT26ND

import os

print(os.listdir(DATASET_DIR))

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(X_train.shape, X_val.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

model = Sequential([
    Conv1D(8, kernel_size=3, activation='relu', input_shape=(100, 13)),
    MaxPooling1D(2),

    Conv1D(16, kernel_size=3, activation='relu'),
    MaxPooling1D(2),

    Flatten(),
    Dense(16, activation='relu'),
    Dense(3, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=32
)

model.save("infant_cry_model.h5")

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open("infant_cry_model.tflite", "wb") as f:
    f.write(tflite_model)

print("TFLite model saved")

def representative_dataset():
    for i in range(100):
        yield [X_train[i:i+1].astype("float32")]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

with open("infant_cry_model_int8.tflite", "wb") as f:
    f.write(tflite_quant_model)

print("INT8 TinyML model saved")

import os

size_kb = os.path.getsize("infant_cry_model_int8.tflite") / 1024
print(f"INT8 model size: {size_kb:.2f} KB")

import numpy as np

interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
scale, zero_point = input_details[0]['quantization']
print(scale, zero_point)

sample = X_val[0:1]

sample_int8 = sample / scale + zero_point
sample_int8 = sample_int8.astype(np.int8)


interpreter.set_tensor(input_details[0]['index'], sample_int8)
interpreter.invoke()

output = interpreter.get_tensor(output_details[0]['index'])
print("Predicted class:", np.argmax(output))